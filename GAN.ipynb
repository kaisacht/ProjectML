{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import random\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "import multiprocessing\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import json\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix\n",
    "from gensim.models import Word2Vec\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_source = pd.read_csv('data_source.csv')\n",
    "#data_source = data_source[data_source['label']=='153 - Input Data Manipulation']\n",
    "data_source, _ = train_test_split(data_source, test_size=0.75, random_state=42)\n",
    "data_target  = pd.read_csv('data_target.csv')\n",
    "#data_target, _ = train_test_split(data_target, test_size=0.9, random_state=42)\n",
    "\n",
    "data_target = data_target.sample(frac=1, random_state=42)\n",
    "#data_target = data_target[data_target['label']=='153 - Input Data Manipulation']\n",
    "X_source = data_source['text'].str.replace('/',' ')\n",
    "y_source = data_source['label']\n",
    "X_target  = data_target ['text'].str.replace('/','')\n",
    "y_target  = data_target ['label']\n",
    "rlist =['194 - Fake the Source of Data', '66 - SQL Injection',\n",
    "       '34 - HTTP Response Splitting', '126 - Path Traversal',\n",
    "       '000 - Normal', '272 - Protocol Manipulation',\n",
    "       '310 - Scanning for Vulnerable Software', '242 - Code Injection',\n",
    "       '153 - Input Data Manipulation']\n",
    "mapping = {l: i+1 for i, l in enumerate(rlist)}\n",
    "y_source = [mapping[s] for s in y_source] \n",
    "y_target  = [mapping[r] for r in y_target ]\n",
    "y_source = np.array(y_source)\n",
    "y_target  = np.array(y_target )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33145\n",
      "31356\n"
     ]
    }
   ],
   "source": [
    "print(len(X_source))\n",
    "print(len(X_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at jackaduma/SecBERT were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.03017046311660884\n",
      "0.06034092623321768\n",
      "0.09051138934982653\n",
      "0.12068185246643535\n",
      "0.1508523155830442\n",
      "0.18102277869965305\n",
      "0.21119324181626187\n",
      "0.2413637049328707\n",
      "0.27153416804947955\n",
      "0.3017046311660884\n",
      "0.33187509428269724\n",
      "0.3620455573993061\n",
      "0.3922160205159149\n",
      "0.42238648363252373\n",
      "0.4525569467491326\n",
      "0.4827274098657414\n",
      "0.5128978729823502\n",
      "0.5430683360989591\n",
      "0.573238799215568\n",
      "0.6034092623321768\n",
      "0.6335797254487856\n",
      "0.6637501885653945\n",
      "0.6939206516820033\n",
      "0.7240911147986122\n",
      "0.754261577915221\n",
      "0.7844320410318298\n",
      "0.8146025041484387\n",
      "0.8447729672650475\n",
      "0.8749434303816563\n",
      "0.9051138934982652\n",
      "0.9352843566148741\n",
      "0.9654548197314828\n",
      "0.9956252828480917\n",
      "0.0\n",
      "0.031891822936599055\n",
      "0.06378364587319811\n",
      "0.09567546880979717\n",
      "0.12756729174639622\n",
      "0.15945911468299528\n",
      "0.19135093761959435\n",
      "0.22324276055619338\n",
      "0.25513458349279244\n",
      "0.2870264064293915\n",
      "0.31891822936599057\n",
      "0.3508100523025896\n",
      "0.3827018752391887\n",
      "0.41459369817578773\n",
      "0.44648552111238676\n",
      "0.47837734404898585\n",
      "0.5102691669855849\n",
      "0.542160989922184\n",
      "0.574052812858783\n",
      "0.605944635795382\n",
      "0.6378364587319811\n",
      "0.6697282816685802\n",
      "0.7016201046051792\n",
      "0.7335119275417783\n",
      "0.7654037504783774\n",
      "0.7972955734149764\n",
      "0.8291873963515755\n",
      "0.8610792192881745\n",
      "0.8929710422247735\n",
      "0.9248628651613726\n",
      "0.9567546880979717\n",
      "0.9886465110345707\n"
     ]
    }
   ],
   "source": [
    "model_name = 'jackaduma/SecBERT'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "X_source = list(X_source)\n",
    "X_target = list(X_target)\n",
    "input_texts_source = X_source\n",
    "input_text_target = X_target\n",
    "\n",
    "X_train = []\n",
    "X_test = []\n",
    "\n",
    "for i , text in enumerate(input_texts_source):\n",
    "\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "    input_tensor = torch.tensor([token_ids])\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_tensor)\n",
    "\n",
    "    last_hidden_state = outputs.last_hidden_state\n",
    "\n",
    "    first_token_features = last_hidden_state[0, 0, :]\n",
    "\n",
    "    X_train.append(first_token_features)\n",
    "    if i%1000==0 :\n",
    "        print(i/len(X_source))\n",
    "\n",
    "for j , text in enumerate(input_text_target):\n",
    "\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "    input_tensor = torch.tensor([token_ids])\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_tensor)\n",
    "\n",
    "    last_hidden_state = outputs.last_hidden_state\n",
    "\n",
    "    first_token_features = last_hidden_state[0, 0, :]\n",
    "\n",
    "    X_test.append(first_token_features)\n",
    "    if j%1000==0 :\n",
    "        print(j/len(X_target))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = torch.stack(X_test)\n",
    "X_train = torch.stack(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_target = np.where(np.logical_or(y_target == 8, y_target == 9), 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Vectorizer:\n",
    "#     def __init__(self, method='BOW', ngram_range=(1, 1), max_features=300, emb_fname='', word_index_fname=''):\n",
    "#         self.method = method\n",
    "#         if self.method == 'BOW':\n",
    "#             self.vectorizer = CountVectorizer(analyzer='word', input='content', ngram_range=ngram_range, max_features=max_features)\n",
    "#         elif self.method == 'TFIDF':\n",
    "#             self.vectorizer = TfidfVectorizer(analyzer='word', input='content', max_features=max_features)\n",
    "#         elif self.method == 'Word2Vec':\n",
    "#             self.max_features = max_features\n",
    "#             self.emb_fname = emb_fname\n",
    "#             self.word_index_fname = word_index_fname\n",
    "#         else:\n",
    "#             raise ValueError('Feature extraction method does not exist.')\n",
    "\n",
    "#     def feature_extraction(self, X_train, X_test):\n",
    "#         train_data = self.vectorizer.fit_transform(X_train).toarray()\n",
    "#         test_data = self.vectorizer.transform(X_test).toarray()\n",
    "#         return train_data, test_data\n",
    "\n",
    "#     def get_word_index(self):\n",
    "#         word2id = json.load(open(self.word_index_fname, 'r'))\n",
    "#         return word2id\n",
    "\n",
    "#     def get_embedding_matrix(self):\n",
    "#         np.random.seed(0)\n",
    "#         word2id = self.get_word_index()\n",
    "#         embedding_matrix = np.random.uniform(-0.25, 0.25, [len(word2id) + 1, self.max_features])\n",
    "#         with open(self.emb_fname, 'r', encoding='utf-8') as f:\n",
    "#             for line in f:\n",
    "#                 content = line.split(' ')\n",
    "#                 if content[0] in word2id:\n",
    "#                     embedding_matrix[word2id[content[0]]] = np.array(list(map(float, content[1:])))\n",
    "#         return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizer = Vectorizer('BOW')\n",
    "# X_train, X_test = vectorizer.feature_extraction(X_source, X_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# sentences = [sentence.split() for sentence in X_source]\n",
    "# w2v_model = Word2Vec(sentences, window=5, min_count=5, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def vectorize(sentence):\n",
    "#     words = sentence.split()\n",
    "#     words_vecs = [w2v_model.wv[word] for word in words if word in w2v_model.wv]\n",
    "#     if len(words_vecs) == 0:\n",
    "#         return np.zeros(100)\n",
    "#     words_vecs = np.array(words_vecs)\n",
    "#     return words_vecs.mean(axis=0)\n",
    "\n",
    "# X_train = np.array([vectorize(sentence) for sentence in X_source])\n",
    "# X_test = np.array([vectorize(sentence) for sentence in X_target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"\"\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = X_train.shape[1]\n",
    "input_size = input_layer\n",
    "hidden_size =64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_layer, 512),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Dropout(0.3),\n",
    "            # nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), input_layer)\n",
    "        output = self.model(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = Discriminator().to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_layer, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(1024, input_layer),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.model(x)\n",
    "        output = output.view(x.size(0), input_layer)\n",
    "        return output\n",
    "\n",
    "generator = Generator().to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.00001\n",
    "num_epochs = 1000\n",
    "batch_size= 32\n",
    "loss_function = nn.BCELoss()\n",
    "optimizer_discriminator = torch.optim.Adam(discriminator.parameters(), lr=lr)\n",
    "optimizer_generator = torch.optim.Adam(generator.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_source = X_train.to(device=device, dtype=torch.float32)\n",
    "X_target = X_test.to(device=device, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_source = torch.from_numpy(X_train).to(device=device, dtype=torch.float32)\n",
    "# X_target = torch.from_numpy(X_test).to(device=device, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0.0 Loss D.: 0.6833779811859131\n",
      "Epoch: 0.0 Loss G.: 0.6744371056556702\n",
      "Epoch: 1.0 Loss D.: 0.5643945932388306\n",
      "Epoch: 1.0 Loss G.: 0.5600152015686035\n",
      "Epoch: 2.0 Loss D.: 0.5812645554542542\n",
      "Epoch: 2.0 Loss G.: 0.5743303298950195\n",
      "Epoch: 3.0 Loss D.: 0.5351682305335999\n",
      "Epoch: 3.0 Loss G.: 0.6797955632209778\n",
      "Epoch: 4.0 Loss D.: 0.48746925592422485\n",
      "Epoch: 4.0 Loss G.: 0.6629167199134827\n",
      "Epoch: 5.0 Loss D.: 0.4519200026988983\n",
      "Epoch: 5.0 Loss G.: 0.6935461163520813\n",
      "Epoch: 6.0 Loss D.: 0.46945640444755554\n",
      "Epoch: 6.0 Loss G.: 0.6937960982322693\n",
      "Epoch: 7.0 Loss D.: 0.42401832342147827\n",
      "Epoch: 7.0 Loss G.: 0.7102949619293213\n",
      "Epoch: 8.0 Loss D.: 0.4297463595867157\n",
      "Epoch: 8.0 Loss G.: 0.6816540956497192\n",
      "Epoch: 9.0 Loss D.: 0.4027346074581146\n",
      "Epoch: 9.0 Loss G.: 0.6937461495399475\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    idx = np.random.randint(0, X_source.shape[0], batch_size)\n",
    "    real_samples  = X_source[idx]\n",
    "    real_samples_labels = torch.ones((batch_size,1)).to(device=device)\n",
    "    latent_space_samples = torch.rand((batch_size,input_layer)).to( device=device)\n",
    "    \n",
    "    generated_samples = generator(latent_space_samples)\n",
    "    generated_samples_labels = torch.zeros((batch_size, 1)).to(\n",
    "            device=device\n",
    "        )\n",
    "    all_samples = torch.cat((real_samples, generated_samples))\n",
    "    all_samples_labels = torch.cat(\n",
    "            (real_samples_labels, generated_samples_labels)\n",
    "        )\n",
    "\n",
    "        # Training the discriminator\n",
    "    discriminator.zero_grad()\n",
    "    \n",
    "    output_discriminator = discriminator(all_samples)\n",
    "    all_samples_labels = all_samples_labels.view(-1, 1)\n",
    "    loss_discriminator = loss_function(\n",
    "            output_discriminator, all_samples_labels\n",
    "        )\n",
    "    loss_discriminator.backward()\n",
    "    optimizer_discriminator.step()\n",
    "\n",
    "        # Data for training the generator\n",
    "    latent_space_samples = torch.rand((batch_size,input_layer)).to(device=device)\n",
    "\n",
    "        # Training the generator\n",
    "    generator.zero_grad()\n",
    "    generated_samples = generator(latent_space_samples)\n",
    "    output_discriminator_generated = discriminator(generated_samples)\n",
    "    loss_generator = loss_function(\n",
    "            output_discriminator_generated, real_samples_labels\n",
    "        )\n",
    "    loss_generator.backward()\n",
    "    optimizer_generator.step()\n",
    "    if(epoch%100==0):\n",
    "        print(f\"Epoch: {epoch/100} Loss D.: {loss_discriminator}\")\n",
    "        print(f\"Epoch: {epoch/100} Loss G.: {loss_generator}\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_discriminator.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = discriminator(X_target)\n",
    "new_tensor = torch.where(predicted_labels >= 0.5, torch.tensor(1), torch.tensor(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_label = new_tensor.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6468937364459753"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_target, new_label)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.6468937364459753\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.02      0.00      0.00     10948\n",
      "           1       0.65      0.99      0.79     20408\n",
      "\n",
      "    accuracy                           0.65     31356\n",
      "   macro avg       0.33      0.50      0.39     31356\n",
      "weighted avg       0.43      0.65      0.51     31356\n",
      "\n",
      "Confusion Matrix: \n",
      " [[    2 10946]\n",
      " [  126 20282]]\n"
     ]
    }
   ],
   "source": [
    "print('accuracy =', accuracy_score(y_target, new_label))\n",
    "print(classification_report(y_target,new_label))\n",
    "print('Confusion Matrix: \\n',confusion_matrix(y_target, new_label))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
